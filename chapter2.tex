\chapter{Literature Review and Theoretical Framework}
\label{chap:literature}

\section{Conceptualizing Enrollment Demand and Institutional Planning}
\label{sec:conceptualizing}

Enrollment demand refers to the number of students that both new and returning students are willing and able to participate in under the current demographic, economic, and institutional situations. In higher education administration, demand cannot be seen as a single physical measure. It is deduced from a series of administrative and behavioral indicators that illustrate the flow of students through the admissions and persistence pipeline, including inquiries, applications, admits, deposits, registrations, and eventually headcount or full-time equivalent enrollment. Each indicator reflects a different decision margin and is therefore associated with a different managerial problem \citep{hossler1987, perna2006}.

A major conceptual difference is that enrollment can be seen either as a stock or as a flow. A census headcount at a specified reporting date is a stock variable that results from current-student progression, retention, transfer, and stop-out patterns. New entrants, however, are a flow that is more directly affected by changes in the market and recruitment activities. In fact, forecasts of intake and headcount/full-time equivalents are just two examples of a more extensive range of forecasting tasks in practice. An intake forecast is a tool to plan for the coming academic year in terms of staff, housing, and course scheduling, while headcount and full-time equivalent forecasts are needed for multi-year budgeting and capacity planning \citep{tinto1975, bean1980}.

Economic theories of educational choice generally go human-capital way first by considering education as an investment and hence if expected returns must be greater than not only direct costs but opportunity costs as well. According to such a view, demand is affected by that expectations of earnings, probabilities of employment, and the arrangement of costs and benefits over time. However, studies of student decision-making done so far have consistently shown that information constraints and behavioral mechanisms play a significant role in the determination of the choices that are made. Students and families deal with complicated pricing schemes, they have guesses as to what their financial aid will be, and there are variations in the beliefs about the quality of different institutions that exist. These frictions, therefore, provide a rationale for the fact that even under similar macroeconomic conditions, institutions that appear to be extremely similar can exhibit different yield patterns \citep{becker1964, mincer1974, dynarski2003, bettinger2012}.

Higher education institutions do not simply respond to demand with a lag. They deliberately employ strategic tools such as tuition pricing, financial aid, and admission selectivity with a view to maximizing their budget and advancing their reputational goals, and these tools are frequently tweaked before the institution sees the need to exploit them. This co-determination entails a feedback loop: the enrollment results that are seen not only come from an external demand but also from the institutional response to the earlier demand signals. For forecasting, the theoretically most relevant view is the pragmatic one. The goal is to utilize the stable regularities of the joint system that result in accurate predictions at the decision-making time while being aware that the coefficients obtained from the analysis of observational administrative data generally reflect associations rather than causal effects \citep{ehrenberg2000, angrist2009}.

At last, customer needs are divided. Each of two-year colleges, regional public universities, and selective private institutions is different in marketing, serves other groups of students, and has different limitations. Community colleges usually react quickly to changes in the local economy and an increase in the number of adult learners, while selective institutions may be more limited by the number of available spots and admissions policy than by the changes in the number of applicants. So, a theoretical framework for enrollment forecasting represents the heterogeneity by sector, selectivity, mission, modality, and geography, and it either encourages stratified analysis or models that reveal differences across types of institutions \citep{grawe2018, bailey2015, barr2013}.

\section{Determinants of Enrollment Demand}
\label{sec:determinants}

The factors that influence enrollment demand are often categorized into demographic, economic, financial, institutional, and contextual ones. Demography determines the basic limits of participation. The number of young people in the traditional college-age cohort, migration trends, and adult learner participation together determine the pool of potential students. On a regional level, population change ranks among the most consistent factors of demand, but its influence depends on the extent of the institution's reach. Institutions whose student base comprises mainly their local commuting areas are more vulnerable to local cohort decreases than institutions with national recruitment or specialized program offerings \citep{grawe2018, perna2006}.

Enrollment is affected by economic conditions via opportunity cost and resource channels. A rise in job opportunities and wages means that the value of time used in education goes up, so the immediate enrollment of some groups might be reduced. On the other hand, in a recession, enrollment may go up as people reskill or postpone entering the labor market. The direction and extent of the correlation differ depending on age, level of credential, and local industry structure. To illustrate, short-cycle programs can be very quickly influenced by local unemployment, whereas the demand for long-cycle degrees may be also term less elastic and more influenced by expectations of longer-term returns \citep{barr2013, long2014}.

Affordable mechanisms refer to connecting prices, aids, and liquidity constraints with enrollment that has been observed. In theory, the net price after grants should be the price margin that is relevant. However, when the aid information is uncertain or difficult to understand, students may rely on the published tuition and fees. Need-based grants, merit aid, and state scholarship policies change the effective price distribution that students face and thus have both application volume and yield in their influence. On the institutional side, discounting decisions are limited by tuition dependence, endowment resources, and state appropriations. These issues lead to the use of financial aid and net price indicators in empirical demand models, usually lagged to indicate information availability at the time of enrollment decisions \citep{dynarski2003, bettinger2012, hoxby2015}.

Institutional characteristics act as factors through which demand is influenced by perceived quality, program fit, and student support capacity. Control and sector are determining factors of mission differences, pricing regimes, and regulatory environments. Selectivity and admissions pressure measures are indicative of both applicant sorting and institutional constraints. Staffing and student-faculty ratios serve as a means of measuring instructional resources and could possibly be used to account for perceived academic experience as well. Besides that, program mix is a fundamental aspect: demand changes when institutions qualify in high-return fields or offer flexible modes such as online or hybrid delivery that minimize time and location restrictions \citep{ehrenberg2000, allen2017}.

Contextual factors think about policy environments and shocks. State tuition caps, changes in need-based aid, and federal regulations can affect affordability and institutional strategy. Shocks like at public health can cause discontinuities in patterns of history by changing preferences, constraints, and labor-market trajectories. The theoretical implication here is that models are not required to measure every shock explicitly. Instead, models should be capable of handling instability through benchmarking, a parsimonious specification, and evaluation protocols that reveal how performance drops off when conditions differ \citep{kane1995, aucejo2020}.

\subsection{Conceptual Framework}
\label{subsec:conceptual-framework}

Figure~\ref{fig:conceptual-framework} depicts a conceptual framework that institutions rely on to forecast and plan around enrollment outcomes. The framework shows how different factors from the institutional, economic and contextual spheres have a direct or indirect influence on enrollment demand through several channels. These channels eventually come down to an observable enrollment outcome.

\begin{figure}[htbp]
\centering
% Include the TikZ diagram here if available
% \input{b_chapters/chapter2/assets/conceptual_framework_tikz.tex}
% Or placeholder for the diagram
\fbox{\parbox{0.8\textwidth}{\centering [Conceptual Framework Diagram]\\
Institutional Factors $\rightarrow$ Enrollment Demand\\
Economic Factors $\rightarrow$ Enrollment Demand\\
Contextual Factors $\rightarrow$ Enrollment Demand}}
\caption{Conceptual Framework of Enrollment Demand Determinants}
\label{fig:conceptual-framework}
\end{figure}

The model brings together the two sets of theories explored earlier in Sections~\ref{sec:conceptualizing} and~\ref{sec:determinants}. It illustrates how the demand for enrollment is a result of the coming together of student decision-making processes (affected by the socioeconomic conditions and money matters), institutional strategic actions (admission funnel management, pricing, financial aid), and external contextual factors such as the policy environment and demographic trends. The model basically conveys that enrollment is not just a matter of student preferences or institutional capacity but rather the result of the interplay of these various factors. This theoretical framework paves the way for the empirical study by showing which variables in administrative datasets can be used as indirect measures of the grounding theoretical concepts.

\section{Administrative Data Sources and Measurement Considerations}
\label{sec:admin-data}

Administrative data appeal to enrollment prediction experts because of their extensive coverage, compatibility between institutions, and the availability of consistent longitudinal identifiers. On the other hand, administrative datasets are a reflection of reporting conventions thus need to be cautiously interpreted. Strategic forecasting and benchmarking across sectors can be done with institution-level panels, but they lack the capturing of a school's internal heterogeneity for example program-level substitution, differing major retention rates, or the student subpopulations that have varying responses to the intervention. These limitations are considerable. They determine the types of questions that can be answered with great credibility and the kinds of conclusions that should be avoided \citep{angrist2009, nces2024}.

The Integrated Postsecondary Education Data System (IPEDS), a product of the National Center for Education Statistics (NCES) is the main federal data collection for college statistics and is widely used in empirical studies due to the fact that it standardizes reporting and offers stable identifiers. IPEDS segments cover admissions and enrollment data, institutional characteristics, and financial assistance-related indicators. The survey parts, however, may vary in their extent and relevance. The admissions variables are mainly aimed at institutions with selective admissions and might not include institutions with open admission or nonstandard reporting frameworks. In the same way, some financial aid variables are established for certain student groups or reporting years, and the timing of their release can be behind the decision context \citep{nces2024}.

The U.S.\ Department of Education's College Scorecard highlights other factors related to results and affordability, for example, completion rates, and a few indicators tied to debt and earnings that come from it. These factors can help descriptive analysis to a greater extent if the link between demand and the returns as well as the institutional performance is established. However, at the same time, they pose certain difficulties: there are differences in cohort definitions, some cells are not disclosed in order to ensure privacy, and a number of outcome measures are basically reflecting past cohorts rather than present conditions. The key methodological aspect for forecasting is that of such variables, only lagged predictors can be inputs, and the interpretation has to take into account the temporal ordering between what students recall could be known to them at the time of decision and what is actually coming from retrospective administrative outcomes \citep{collegecard2024}.

Measurement challenges are not limited to just missing data. For example, an institution can be merged, closed, or change its control status; there can be shifts in reporting practices; changes to the definitions can lead to discontinuities. Such activities can produce breaks of structure in time series and thus entail explicit data cleaning, which also includes deduplication and consistent aggregation to a single institution-year observation. Count variables usually also are very skewed, which results in transformations and the use of robust evaluation metrics that are not heavily influenced by a small number of large institutions \citep{nces2024, perron1989}.

One of the common issues is that a few administrative records could be arranged by subgroups, e.g., sex or level of study, thus resulting in multiple institution-year entries. Developing an analysis-ready panel from this data calls for a well-founded aggregation procedure in line with the theoretical construct of interest. Suppose enrollment demand is reflected as an incoming cohort, then it would be consistent to aggregate subgroup counts to a total. If the interest is in composition, then perhaps subgroup-specific panels will be necessary. Hence, clear and thorough recording of aggregation, suppression handling, and variable definitions is not just an add-on to the modeling. This is a part of the theoretical operationalization of demand \citep{nces2024}.

\section{Forecasting Approaches for Enrollment Time Series}
\label{sec:forecasting-approaches}

Forecasting methods vary in complexity from simple univariate extrapolation models to multivariate and structurally based models. Univariate methods base their prediction on the assumption that the future will be similar to the past and are thus necessary as benchmark models because enrollment time series usually have a very strong autocorrelation component. The naive forecast, which predicts that the next value will be the same as the last observed one, may be very hard to beat in the case of stable institutions only. Moving average forecasting techniques minimize the effect of a noise of a single year by smoothing the most recent observations, and they can be quite efficient when the underlying level changes slowly in comparison with the year-to-year variability \citep{hyndman2021, makridakis2018}.

Exponential smoothing extends moving averages by giving geometrically decaying weights to the past observations. In the state-space form, smoothing methods can be seen as latent level and trend components, which evolve over time with stochastic disturbances. This approach is very suitable for annual enrollment as it allows the model to adjust systematically to rising or falling trends gradually while still being very simple. When the series are short, simplicity is not just a matter of style. It is actually a necessity because complicated models may simply fit the noise in historical data rather than the actual structure \citep{gardner1985, durbin2012, hyndman2021}.

Autoregressive integrated moving average (ARIMA) models offer a very adaptable way of representing autocorrelation and stochastic shocks after differencing to get rid of the non-stationarity. The Box-Jenkins approach mainly focuses on diagnostic checking and simple order selection. However, when we talk about institution-level annual data series, the short time dimension restricts the usefulness of high-order ARIMA models. There is, therefore, a theoretically justifiable use of ARIMA in this situation mainly at a higher level, e.g., sector or regional totals, where the effective signal-to-noise ratio gets better and the time series may exhibit more stable autocorrelation structure \citep{box2015, hyndman2021}.

One of the frequently encountered practical problems in enrollment is a structural change. Institutions may change the way they select students, increase the number of online programs, or get a change in the law that will result in the enrollment of past students being less related to the future ones. Besides, exogenous shocks may also be a source of persistence disruption. There is no forecasting method that can completely get rid of the effect of structural breaks, however, models can be assessed and chosen so as to be less risky of overconfidence. That is, amongst others to be done by comparing with very simple baselines, restricting the number of parameters, and planning test windows to also cover the times which are likely to have acted as stress periods \citep{perron1989, hyndman2021}.

The theoretical role of baselines should be stressed. In the area of forecasting, a complicated model gains the respect of a community by showing its superiority over naive and smoothing benchmarks when the evaluation respects the time order. The dominance of baselines is an informative outcome: it points out that persistence contains the main bulk of the predictive content at the institution-year level, and it turns back the research to segmentation, the quest for truly exogenous drivers, or alternative dependent variables that are more in line with the decision context \citep{makridakis2018, hyndman2021}.
\section{Panel Forecasting and Hierarchical Modeling}
\label{sec:panel-forecasting}

Institution-year panels make possible the models that share strength across institutions, thereby raising the effective sample size, and at the same time allow for the inclusion of covariates that represent affordability, admissions pressure, and institutional capacity. Pooling might in fact be a way to gain stability, however, it could also mean averaging over the heterogeneous dynamics. The methodological issue is how much of the pooling is theoretically justified. Complete pooling is generally equivalent to assuming that the same set of predictors explains the demand of all institutions, whereas, no pooling, in which case each institution is seen as an independent time series, becomes largely infeasible due to the short length of time series \citep{wooldridge2010, baltagi2005}.

Panel econometrics gives us a means to talk about these trade-offs with the help of fixed effects and random effects models. Through fixed effects we can remove the impact of time-invariant unobserved heterogeneity by concentrating on changes within one specific institution. This approach can be particularly useful if institutions have different unobserved characteristics which are correlated with the observed predictors, e.g.\ mission, local reputation, or historical brand equity. On the other hand, fixed effects can filter out between-institution variation that may actually be useful for prediction, and they may also weaken the signal when the predictors vary slowly over time \citep{wooldridge2010, baltagi2005}.

Dynamic panel specifications feature lagged dependent variables as a way of showing the persistence of enrollment. The use of lagged outcomes usually leads to better predictive performance because a lagged outcome essentially captures a large number of unobserved factors, e.g., the long-run reputation and momentum of the peer group. Nevertheless, dynamic models make it harder for one to check the assumptions of the model as lagged outcomes can be correlated with unobserved shocks. When it comes to forecasting, the main focus is on predictive validity. However, it is still very crucial to make it clear that the coefficients in dynamic observational models are generally descriptive associations conditional on persistence rather than causal effects of policy levers \citep{arellano1991, nickell1981}.

Today's predictive methods advance panel modeling by using regularization and more flexible function classes. It is especially beneficial to use regularized regression methods if the set of predictors is huge or the predictors are highly collinear, which is typically the case in administrative data, where various measures are used as proxies for the same constructs. In addition, tree-based models are able to capture nonlinearities and interactions, such as the extent to which different sectors are affected by price or how nonlinear yield responses are to admissions pressure. Nonetheless, their flexibility necessitates thorough validation in case they take advantage of non-existent structure if the data is sparse or noisy \citep{tibshirani1996, hastie2009, breiman2001}.

It is often the case that segmentation is necessary on theoretical grounds. The factors determining the demand for public two-year colleges are different from those determining the demand for selective private institutions. Hence, a single pooled model across all sectors may weaken the strength of essential relationships and hide patterns that are relevant from an operational standpoint. Ways to make statistical modeling consistent with theoretical heterogeneity that has been described in the literature by stratified estimation, interaction terms, or hierarchical models with parameters allowed to vary by sector \citep{perna2006, grawe2018}.

\section{Driver Analysis and the Limits of Inference}
\label{sec:driver-analysis}

Usually, the institutional stakeholders require not only accurate predictions but also an understanding of which observable factors are linked to the changes in the demand. Driver analysis frequently employs the regression models as a means of relating enrollment results to affordability indicators, admissions pipeline measures, and institutional characteristics. If done thoughtfully, such models offer a regulated account of the association and may facilitate managerial learning through the identification of demand correlates that remain stable across years \citep{ehrenberg2000}.

The distinction between explanation and prediction on the methodological level is a key one. Predictive models are evaluated based on their accuracy on unseen data, whereas explanatory models are assessed by the plausibility of their identifying assumptions and the interpretability of their estimated effects. In the enrollment context, policy variables such as tuition, aid, recruitment intensity, and capacity are usually endogenous. Institutions determine them as a response to their expectations of demand and their fiscal conditions. Therefore, the estimated relationship between pricing metrics and enrollment might be a mix of student responses and institutional responses, and one should refrain from interpreting it straightforwardly as a causal elasticity unless there is a further identification strategy \citep{shmueli2010, breiman2001}.

Machine learning algorithms reveal the same set of explanatory factors as traditional methods, but they don't surpass these in providing evidence. The importance of features and the partial dependence plots can show which variables are mainly predictive and also reveal the non-linearities or interactions of these variables. Nevertheless, such indicators can be influenced by the fact that there are correlated predictors, measurement errors, and missingness patterns. A variable that is important might be a proxy for a hidden factor, might be less noisy, or might be so correlated with the lagged target that it reflects it. Therefore, the results of a driver analysis based on ML should be considered as plain, descriptive, and explorative, and not as a replacement of the causal inference \citep{molnar2022, strobl2007}.

One of the defensible methods is triangulation. Baseline models determine the level of persistence strength. Regularized regression provides minimal association results with controlled complexity. Flexible models adjust to the idea that relationships vary among segments or that effects are nonlinear. Reliable conclusions are those that remain valid after several tested hypotheses and that are consistent with the understanding of enrollment processes and institutional constraints \citep{shmueli2010, hyndman2021}.

In brief, driver research of administrative enrollment forecasting can be considered quite trustworthy in case the driver research was providing associations as indications and, at the same time, driver research was combined with the forecasting evaluation. Having a clear theory of what exactly the model is estimating and what it is not, plays a major role in making sure that the results of the analysis are rightly interpreted in planning and policy discussions \citep{angrist2009, shmueli2010}.

\section{Forecast Evaluation, Validation, and Reproducibility}
\label{sec:evaluation}

Evaluation of time series and panel forecasting needs to keep the time order of the data. If one cuts the data randomly and assigns some of the data to the training set and some to the test set, the training set will be contaminated with information from the future, especially in the case of long persistence. Therefore, rolling-origin evaluation, which is also known as walk-forward validation, is the method of choice. The model is initially trained on a window of years, for example, and then tested on the next year, after which the training window is either expanded or rolled forward. This method replicates what happens in reality and gives a true picture of how the accuracy of forecasts changes with the availability of new data \citep{bergmeir2012, hyndman2021}.

The metrics used for performance evaluation are a matter of importance. Mean absolute error is outlier insensitive and is straightforward to interpret in the student units, which makes it a good fit for staffing and budgeting decisions. Root mean squared error, on the other hand, punishes larger deviations more heavily and thus, is suitable for cases where large errors in forecasts result in disproportionately high operational costs such as if the number of students who need housing is overestimated or the number of courses is underestimated. Mean absolute percentage error allows for comparing institutions of different sizes on a scale-free basis, but it is volatile when the number of enrollments is low. If percentage errors are given, they should come with precautions, such as leaving out very small denominators or also giving other metrics that are still well behaved for small institutions \citep{hyndman2006, hyndman2021}.

Benchmarking is a must. For instance, a model that fails to better naive or smoothing baselines might still be useful for scenario analysis, descriptive driver exploration, or communication, but it cannot be used to support claims of having superior predictive power. On the other hand, even small gains over strong baselines can be significant when they are summed up across a large number of institutions and years. Hence, publication entails disclosing not only the mean performance but also providing distributional evidence, e.g., the proportion of institutions for which a model leads to an improvement over the baseline or the extremes of the error distribution \citep{makridakis2018}.

Reproducibility becomes even more crucial when dealing with administrative data forecasting as decisions made during data preparation can significantly influence the outcomes. Issues like duplicate records, subgroup division, suppressed values, and changing variable definitions are frequent. A reproducible pipeline not only tracks the origin of the data but also clearly states the criteria for the selection and exclusion of data, keeps the intermediate datasets, and maintains the model settings and evaluation results. This habit facilitates scientific transparency and makes it possible to audit the process when forecasts are used for making important decisions about the allocation of personnel, financial aid budgets, and program investment \citep{peng2011}.

Lastly, assessment results must be understood with decision risk in mind. Organizations typically value least the mean error and most the error at the tail due to the fact that extremal cases of under- or over-forecasting may inflict significantly higher than average costs. Thus, if possible, the evaluation may be carried out with the help of tail metrics, prediction intervals, or scenario stress tests. Also, the main product may be a point forecast but the theory behind it must consider uncertainty and the practical usefulness of conveying it properly \citep{gneiting2014}.

\section{Summary and Implications for the Present Study}
\label{sec:lit-summary}

The reviewed literature indicates that a model has been put forward in which enrollment demand remains high at all times, but it is still sensitive to things like affordability, admissions pressure, and institutional capacity, resulting in the very diverse picture across different sectors and regions. On the one hand, administrative datasets provide an opportunity for large-scale comparative studies, on the other hand, they are also associated with definitional limitations that make it necessary to precisely define the demand construct. Therefore, forecasting methods should be selected based not only on their empirical performance but also on their interpretability, and at the same time, due consideration should be given to the temporal validation protocols that eliminate the possibility of excessively optimistic assessments.

For this research, the reasoning leads to three methodological commitments. Firstly, the dependent variable and aggregation choices have to be selected in a way that they correspond to the planning problem and the definitions stipulated in the federal administrative reporting. Secondly, model evaluation has to be carried out via a walk-forward procedure that genuinely reflects the method of making forecasts and also prevents optimistic bias. Third, the explanatory interpretation of covariates should be considered as correlations rather than causal effects, realizing that institutional decisions and enrollment outcomes are co-determined instead of being strictly sequential.

Based on the theoretical and methodological considerations from the literature, this research intends to implement a framework through a systematic empirical investigation that is organized as follows. Chapter~\ref{chap:methodology} will explain the data construction process description of how the IPEDS panel dataset covering 2010--2021 is put together, cleaned, and prepared for both forecasting and driver analysis. It will outline the exact definitions of variables, procedures for dealing with missing data, and the exploratory patterns that serve as a rationale for the modeling choices. Chapter~\ref{chap:methodology} will also feature the technical execution of the walk-forward validation protocol, the specifications of the baseline and ARIMA models, and the panel regression arrangement for driver evaluation. This clear methodological disclosure allows the empirical results in Chapter~\ref{chap:results} to be understood in the light of the limitations and strengths of administrative data rather than making unsupported claims. By anchoring the methodology strongly in the reviewed literature here primarily the focus on baselines, temporal validation, and interpretability the work sets out to produce enrollment forecasts and driver insights that are not only technically reliable but also institutionally decision-makers practical.
