% Chapter 5: Discussion and Conclusion
% Enrollment Forecasting Thesis

\chapter{DISCUSSION AND CONCLUSION}
\label{ch:discussion}

This chapter is the last one of the dissertation. It interprets the empirical results that were given in Chapter 4, relates them to the literature review done in Chapter 2, and considers their theoretical as well as practical implications. The chapter is broken down into seven parts. In Section 5.1, the author summarizes the key results, connects them with the research questions and hypotheses from Chapter 1, and elaborates on the findings. Section 5.2 is devoted to discussing the theoretical implications of the findings and, e.g., explaining the dominance of persistence in enrollment trends and the difficulties this brings to innovation forecasting. Section 5.3 lays out the practical steps that institutional researchers, enrollment managers, and university administrators can take. Section 5.4 presents the limitations of the work and shows how the results should be interpreted and if they can be generalized seeing those restrictions. Section 5.5 offers suggestions for the future research that may continue and expand the present one at different points. Section 5.6 discusses the methodological aspects of the study. Lastly, Section 5.7 offers concluding remarks on the overall contribution of this research to the field of enrollment forecasting and institutional planning.

\section{Summary of Main Findings}

The research was mainly aimed at solving two issues: (1) What forecasting methods give the most precise enrollment forecasts in a walk-forward validation setting? and (2) What institutional and affordability factors are statistically associated with enrollment demand that is stable over time even after accounting for persistence? The empirical study using a comprehensive panel dataset of U.S. postsecondary institutions from 2010 to 2021 came up with a number of important discoveries that answer the above questions.

First, enrollment is an extremely persistent variable. The panel regression analysis shows that lagged enrollment by itself accounts for about 98\% of the variance in the current enrollment and the coefficient is 0.9847. So, if you look at any given year, about 98.5\% of the institution's enrollment is simply carried over from the previous year, changing only 1.5\% of the whole for the net. Such a high degree of autocorrelation is in line with the previous studies which have observed the inertia of enrollment \citep{bound2007cohort, hossler1987studying} but it is more accurately measured here by the use of longitudinal panel data.

Moreover, second naive persistence---the assumption that next year's enrollment will be the same as the current year---was the most accurate forecasting model across all test years. Naive baseline got an average Mean Absolute Error (MAE) of 39.43 students over four out-of-sample test years (2018--2021), thus it performed better than a 3-year moving average (44.43 MAE), Ridge regression (40.43 MAE), and Random Forest regression (41.29 MAE). Even though performance differences in absolute numbers are small, they are still consistent over the years and remain after additional predictors have been included, therefore leading to the rejection of Hypothesis 1, which predicted that more sophisticated models would surpass the baseline by at least 10\%.

Third, ARIMA time series models of national total enrollments came up with very accurate forecasts at the national level, with percentage errors below 2\% for all test years. Yet this success at the aggregate-level cannot be used for campus-level forecasting, where individual factors and strong persistence effects play a dominant role. The difference between aggregate and institution-level forecast accuracies underscores the need of testing models at the suitable level of aggregation for the purpose at hand.

Fourth, driver analysis based on panel regression showed that admissions funnel metrics---namely, lagged applications and lagged admissions---are statistically significant predictors of enrollment demand. The two independent variables have positive signs ($\beta = 0.0021$ and $\beta = 0.0084$, respectively) with $p < 0.001$, thus confirming the theoretical model of enrollment growth as a function of the applicant pool size and composition. Net price is negatively and significantly (in terms of statistics) associated with demand ($\beta = -0.0003$, $p = 0.021$), thus being in line with the assumption of price sensitivity, however, the magnitude of the effect is minimal. Average grant aid does not seem to correlate significantly with enrollment ($p = 0.432$), which may be explained among other things by endogeneity or measurement difficulties. Overall, these findings partially agree with Hypothesis 2: admissions funnel indicators explain the variation in enrollment whereas the indicators of affordability provide ambiguous evidence.

Fifth, in 2020, the COVID-19 pandemic brought about an extraordinary external shock that resulted in greater forecast errors for all models. Total enrollment dropped by 6.8\% compared to the 2018 high, while error surges at the institution level reached 15--20\%. Notably, the relative ordering of models did not change even in the pandemic: simple persistence still gave better results than other models, implying that the results hold true in the face of external fluctuations and are not only the effects of the pre-pandemic stable conditions.

The main idea from these results is enrollment forecasting should be viewed as the problem of predicting incremental changes against a highly persistent baseline, rather than estimating enrollment levels from scratch. Since persistence dominates, the scope for improvement by adding more predictors or using more flexible functional forms is very limited, at least given the administrative data used here.

\section{Theoretical Implications}

The findings of the study have several implications for the development of theory and conceptual understanding of the enrollment dynamics.

\subsection{Persistence as a Structural Feature of Enrollment}

The extremely high persistence coefficient (0.9847) shown in the study indicates that enrollment is not only a matter of short-term changes and adjustments but is fundamentally persistent due to the influence of institutional and student-level factors. At the institutional level, factors such as enrollment capacity limits, faculty and staff numbers, availability of residence halls, and budget planning provide strong reasons for institutions to keep enrollment steady. If enrollments went up and down dramatically from one year to the next, it would mean that resource levels would have to be adjusted accordingly. Such changes are costly and disruptive. Institutions hence take strategic measures like changing admissions selectivity, tuition discounting, and recruitment intensity to control enrollment and prevent volatility.

Enrollment persistence is, in fact, the reflection of how stable the demand for higher education is over time from a student perspective. Demographic changes, the state of the economy, and the effect of education on earnings generally evolve slowly rather than suddenly, so the stream of students entering higher education establishments has some inertia. Moreover, even external shocks like economic recessions or pandemics, cause temporary fluctuations that are repaired over time as schools and students get used to the new situation.

Structural persistence in this context refers to the fact that the kind of patterns that are important for forecasting remain the same over time. Therefore, the forecasting innovations must demonstrate substantial improvements beyond the strong baseline that is persistence. The models that cannot outperform the naïve persistence are, therefore, not contributing in any meaningful way. It does not matter how theoretically and computationally sophisticated they might be. This result is in line with the general forecasting literature that strongly supports the essential role of simple benchmarks \citep{armstrong2001principles, makridakis2018statistical}. It is therefore concluded that enrollment forecasting should be consequently made as simple as possible rather than being complicated.

\subsection{Limits of Data-Driven Forecasting}

The fact that machine learning models could not beat naive persistence in forecasting points to the inherent limitations of data-driven methods when the process is highly persistent. Although the models used multiple variables and employ flexible functional forms, Ridge regression and Random Forest could hardly beat the baseline. The result implies that most of the information from the admissions funnel, financial aid, and institutional features is already included in the lagged enrollment. Hence, these additional variables have very little predictive power beyond what persistence can explain.

One of the implications of this finding is related to the rise in excitement about the use of machine learning in educational analytics. Though machine learning is great at handling situations with complicated nonlinear relationships and high-dimensional interactions, it seems that predicting enrollments---especially at the level of annual institution---is a field where simple linear persistence mainly rules. Hence, the additional benefits of using more complex models are almost non-existent, and the costs (such as computational burden, interpretability loss, overfitting risk) may well weigh more than the slight increase in accuracy.

This finding supports the principle of parsimony in forecasting: when a simple model performs as well as a complex one, the simple model should be preferred because it is more interpretable, easier to implement, and less likely to overfit \citep{armstrong2001principles}. In the context of enrollment forecasting, the naïve persistence model provides a transparent baseline that requires minimal computational resources and can be easily explained to stakeholders. More complex models, while theoretically appealing, do not provide sufficient empirical gains to justify their adoption for operational forecasting.

\subsection{Reinterpreting the Role of Drivers}

Traditional enrollment management frameworks emphasize the strategic importance of admissions funnel metrics, pricing decisions, and financial aid policies in shaping enrollment outcomes \citep{hossler1987studying, litten1983different}. The panel regression results in this study confirm that these factors are statistically significant predictors of enrollment demand. However, their practical significance must be interpreted in light of the overwhelming dominance of persistence.

The finding that lagged enrollment explains 98\% of the variance means that institutional interventions through admissions, pricing, or aid have limited scope to alter enrollment in the short run. For example, a 10\% increase in grant aid or a 5\% reduction in net price might produce statistically significant effects in regression models but only translate to marginal enrollment changes (perhaps a few dozen students) relative to the baseline of several hundred or thousand enrolled students. This does not mean that enrollment management is ineffective, but rather that its effects operate on longer time horizons than one-year-ahead forecasts can capture.

From a theoretical standpoint, this suggests that enrollment dynamics should be understood as having two distinct components: a persistent structural component driven by institutional capacity, reputation, and market position, and a marginal adjustment component driven by short-term policy levers. Most of the action in enrollment forecasting occurs in the persistent component, which is well-captured by lagged enrollment. Policy interventions primarily affect the marginal component, which is inherently difficult to predict with high precision because it reflects the cumulative impact of many small decisions and external factors.

This reinterpretation aligns with recent perspectives in enrollment management that emphasize long-term strategic positioning over short-term tactical adjustments \citep{bean1990strategic}. Institutions that wish to grow enrollment substantially cannot rely solely on incremental changes in admissions or pricing policies; they must make strategic investments in program development, facilities, marketing, and student services that shift the institution's position in the competitive landscape over multiple years.

\section{Practical Implications}

These findings offer several practical options for institutional researchers, enrollment managers, and university administrators.

\subsection{Embrace Simple Baselines in Operational Forecasting}

The first practical recommendation is that institutions should adopt simple persistence-based forecasting models for operational enrollment planning. Given that naïve persistence outperforms more complex methods, there is little justification for investing heavily in machine learning infrastructure or sophisticated statistical models for one-year-ahead enrollment forecasts. A simple model that assumes next year's enrollment will equal this year's enrollment provides a robust and interpretable baseline that can be updated easily as new data become available.

This does not mean that institutions should abandon data-driven approaches entirely. Rather, they should use simple baselines as the starting point and reserve more complex methods for scenarios where they demonstrably add value. For example, institutions might use machine learning to identify at-risk students for retention interventions or to optimize financial aid allocation, but these applications are distinct from aggregate enrollment forecasting.

\subsection{Focus Strategic Interventions on Long-Term Trends}

Since short-term enrollment changes are difficult to predict and control, institutions should focus their strategic planning on long-term trends rather than year-to-year fluctuations. Multi-year enrollment goals should be set based on demographic projections, program expansion plans, and competitive positioning rather than on the assumption that enrollment can be fine-tuned through annual adjustments in admissions or pricing policies.

For example, an institution seeking to grow enrollment by 20\% over five years should develop a strategic plan that includes investments in new academic programs, enhanced student services, expanded housing capacity, and targeted recruitment in underserved markets. Such a plan acknowledges that enrollment growth is a long-term process that requires sustained effort and cannot be achieved through incremental policy changes alone.

\subsection{Use Forecast Intervals and Scenario Planning}

Given the uncertainty inherent in enrollment forecasting, especially during periods of external shocks like the COVID-19 pandemic, institutions should complement point forecasts with forecast intervals and scenario analyses. A point forecast (e.g., ``we expect 500 students next year'') conveys a false sense of precision. A more realistic approach would be to provide a range (e.g., ``we expect between 480 and 520 students with 80\% confidence'') and to develop contingency plans for best-case and worst-case scenarios.

Also, organizations can think ahead using scenario planning and take into account events that have very low probability but high impact which are impossible to anticipate with models, examples would include pandemics, economic recessions, or policy changes. Institutions in which there are, for example, three scenarios (base case, optimistic, pessimistic), can organize their budgets and resource allocation to be more responsive to variable enrollment outcomes. Such a method is more in line with the understanding that forecasting is naturally uncertain and needs to have the ability to adapt rather than expecting exactness.

\subsection{Invest in Data Infrastructure for Timely Updates}

Although simple models perform well, their effectiveness depends on having timely and accurate data on current enrollment. Institutions should invest in data infrastructure that enables real-time monitoring of enrollment metrics, including applications, admissions, deposits, and enrollments. Automated dashboards that update daily or weekly can provide early warning signals of deviations from expected trends, allowing administrators to respond proactively.

For example, if deposit rates in April are running 10\% below the previous year, this signal can prompt targeted outreach to admitted students or adjustments to financial aid offers. While such interventions may not dramatically alter overall enrollment, they can help institutions avoid worst-case scenarios and maintain enrollment stability. The key is to combine simple forecasting models with agile data systems that enable rapid response to emerging trends.

\section{Limitations of the Study}

While this study provides valuable insights into enrollment forecasting, several limitations must be acknowledged to contextualize the findings and guide their interpretation.

\subsection{Geographic and Institutional Scope}

The study focuses exclusively on U.S. degree-granting postsecondary institutions that participate in Title IV federal student aid programs and report data to IPEDS. This scope excludes non-degree-granting institutions, private training providers, and international institutions. As a result, the findings may not generalize to other educational contexts where enrollment dynamics differ.

For example, enrollment patterns in European higher education systems with centralized admissions processes (e.g., the UK's UCAS system) may exhibit different levels of persistence and responsiveness to institutional policies. Similarly, for-profit institutions and bootcamp-style training programs that operate on rolling admissions and short program cycles may experience more volatile enrollment patterns than traditional nonprofit institutions. Future research should test whether the dominance of persistence holds in these alternative contexts.

\subsection{Temporal Aggregation and Forecasting Horizon}

The study uses annual data and focuses on one-year-ahead forecasts. This temporal aggregation may mask important within-year dynamics, such as seasonal variation in enrollment or differences between fall and spring intake. Additionally, the one-year forecasting horizon may not be sufficient for long-term strategic planning, which often requires projections three to five years into the future.

Higher-frequency data (e.g., monthly enrollment counts or weekly application data) might reveal patterns that are obscured in annual aggregates. For instance, institutions with significant mid-year entry or summer enrollment might benefit from models that explicitly account for within-year seasonality. Similarly, multi-year forecasts might require different modeling approaches that account for demographic trends, program lifecycle effects, and competitive dynamics that unfold over longer horizons.

\subsection{Causal Inference and Endogeneity}

The panel regression models in this study identify statistical associations between predictors and enrollment outcomes but do not establish causal relationships. Endogeneity is a pervasive concern in observational studies of enrollment because many institutional decisions (e.g., tuition pricing, financial aid allocation, admissions selectivity) are made in response to expected enrollment outcomes, creating reverse causality.

For example, institutions that anticipate lower enrollment may increase financial aid generosity or reduce admissions selectivity, leading to a negative observed correlation between aid and enrollment even if the true causal effect of aid is positive. Similarly, institutions facing enrollment declines may cut tuition or expand recruitment efforts, confounding the interpretation of price and marketing effects. Without exogenous variation or natural experiments, it is difficult to disentangle these endogenous relationships.

Causal inference methods such as instrumental variables, regression discontinuity designs, or difference-in-differences analysis could address these concerns but require specific data features (e.g., policy discontinuities, exogenous shocks) that are not universally available. Future research should explore these methods to provide more definitive causal estimates of enrollment drivers.

\subsection{Model Selection and Hyperparameter Tuning}

The study evaluates a limited set of forecasting models (naïve persistence, moving average, ARIMA, Ridge regression, Random Forest) with standard hyperparameter settings. It is possible that alternative models or more extensive hyperparameter optimization could yield better performance. For example, gradient boosting methods (e.g., XGBoost, LightGBM) or deep learning models (e.g., recurrent neural networks) might capture complex temporal patterns that the models tested here do not.

However, extensive hyperparameter tuning also carries the risk of overfitting to the validation set, which could inflate apparent performance gains that do not generalize to truly unseen data. The walk-forward validation approach used in this study mitigates this risk by reserving a completely independent test set for final evaluation, but the limited number of test years (four) reduces statistical power to detect small but meaningful performance differences.

\subsection{Missing Data and Measurement Error}

Approximately 22\% of institution-year observations in the IPEDS data have missing values for the enrollment outcome variable, and missing data rates are even higher for some predictor variables. While the study excludes observations with missing outcomes, this approach may introduce selection bias if missingness is non-random. For example, smaller institutions and those that do not participate in federal student aid programs are more likely to have missing data, potentially limiting the generalizability of the findings to these populations.

Additionally, measurement error in IPEDS data is a known concern. Institutions may misreport or update their data retrospectively, leading to inconsistencies across survey years. While IPEDS has quality control procedures to minimize such errors, they cannot be entirely eliminated. Measurement error in predictor variables can attenuate regression coefficients and reduce the apparent importance of drivers, potentially understating their true relationships with enrollment.

\section{Directions for Future Research}

The findings of this study suggest several promising directions for future research that could extend and deepen our understanding of enrollment forecasting.

\subsection{Causal Analysis of Enrollment Drivers}

Future research should employ causal inference methods to move beyond correlational findings and identify the true causal effects of institutional policies on enrollment outcomes. Natural experiments, such as policy changes affecting specific states or institution types, provide opportunities to estimate causal effects using difference-in-differences or synthetic control methods. For example, changes in state financial aid programs or tuition caps could be leveraged to identify the price elasticity of enrollment demand.

Instrumental variables approaches might also be fruitful if valid instruments can be identified. For instance, exogenous shocks to institutional budgets (e.g., state funding cuts) could serve as instruments for tuition or aid decisions, enabling estimation of their causal effects on enrollment. Regression discontinuity designs could be applied in contexts where admissions or aid eligibility is determined by arbitrary thresholds (e.g., test score cutoffs), providing clean identification of treatment effects.

\subsection{Higher-Frequency and Real-Time Forecasting}

Most enrollment forecasting studies, including this one, rely on annual data because that is the frequency at which IPEDS and similar administrative datasets are published. However, institutions have access to higher-frequency data on applications, admissions, deposits, and enrollments that could enable more timely forecasting. Future research should explore whether weekly or monthly data improve forecast accuracy and whether real-time updating of forecasts as new information arrives provides actionable insights for enrollment managers.

Nowcasting techniques, which combine high-frequency indicators with formal statistical models, could be particularly valuable for enrollment management. For example, Google search trends, social media activity, or website traffic data might serve as leading indicators of enrollment demand, allowing institutions to update their forecasts continuously rather than waiting for annual data releases.

\subsection{International and Comparative Studies}

While this study focuses on U.S. institutions, enrollment dynamics in other countries may differ due to variations in higher education systems, funding models, and student mobility patterns. Comparative studies that apply similar forecasting methods to data from the UK, Germany, Australia, or developing countries would help assess the generalizability of the persistence finding and identify whether certain institutional or policy contexts enable better forecast performance.

International students represent a particularly interesting subpopulation because their enrollment decisions may be more responsive to policy changes (e.g., visa regulations) and economic conditions (e.g., exchange rates) than domestic students. Forecasting international enrollment may require specialized models that account for these factors and incorporate higher-frequency data on application trends and geopolitical developments.

\subsection{Incorporation of External Data Sources}

This study relies exclusively on IPEDS administrative data, but external data sources could enhance forecast accuracy or provide complementary insights. For example, demographic projections from the U.S. Census Bureau could inform long-term enrollment forecasts by accounting for cohort size effects. Economic indicators such as unemployment rates, wage growth, or college wage premiums might help predict cyclical variation in enrollment demand.

Alternative data sources, such as web scraping of institutional websites, social media sentiment analysis, or crowdsourced rankings, could provide real-time signals of institutional reputation and student interest. Integrating these diverse data streams into forecasting models presents methodological challenges but could yield more robust and timely predictions.

\subsection{Longer Forecast Horizons and Demographic Projections}

While one-year-ahead forecasts are valuable for operational planning, institutions also require longer-horizon projections (three to five years) for strategic planning, capital budgeting, and program development. Extending the forecasting horizon introduces additional uncertainty because more distant outcomes are influenced by factors that are difficult to predict, such as demographic shifts, technological change, and policy reforms.

Demographic projections are particularly important for understanding long-term enrollment trends. The declining birth rates observed in many developed countries imply shrinking cohorts of traditional college-age students, which will intensify competition for enrollment. Institutions located in regions with favorable demographic trends (e.g., growing immigrant populations) may face different enrollment trajectories than those in regions with aging populations. Incorporating spatial and demographic heterogeneity into enrollment forecasting models could improve their relevance for strategic planning.

\section{Methodological Contributions}

Beyond its substantive findings, this study makes several methodological contributions to the enrollment forecasting literature. First, it employs a rigorous walk-forward validation protocol that evaluates forecast performance on completely independent test data. Many prior studies rely on in-sample fit statistics (e.g., R-squared) or single-split validation, which can overstate model performance due to overfitting or data leakage. The walk-forward approach ensures that models are tested under realistic conditions where future data are unknown.

Second, the study compares a diverse set of forecasting methods, ranging from simple heuristics (naïve persistence, moving average) to statistical time-series models (ARIMA) to machine learning methods (Ridge regression, Random Forest). This comprehensive evaluation allows for a fair assessment of relative performance and avoids the pitfall of cherry-picking models that happen to perform well in a particular context.

Third, the study explicitly considers the role of structural breaks, such as the COVID-19 pandemic, in forecast evaluation. By testing models across both stable and disruptive periods, the analysis provides insights into model robustness and the limits of forecasting during extreme events. This is particularly relevant for practical applications, where decision-makers must plan for uncertainty and the possibility of unforeseen shocks.

Fourth, the combination of predictive modeling (forecasting) and explanatory modeling (driver analysis) provides a more complete picture of enrollment dynamics than either approach alone. While the forecasting analysis identifies the limits of prediction, the driver analysis elucidates the factors that shape enrollment outcomes and their relative importance. This integrated approach is well-suited to answering both ``what will happen?'' (forecasting) and ``why does it happen?'' (explanation) questions.

\section{Conclusion}

This dissertation set out to answer two fundamental questions about enrollment forecasting in U.S. higher education: Which forecasting methods are most accurate? And which institutional and affordability factors drive enrollment demand? Using a comprehensive panel dataset spanning 2010 to 2021 and covering nearly 90,000 institution-year observations, the study provides robust evidence on both questions.

The central finding is that enrollment is an extraordinarily persistent variable, with lagged enrollment accounting for 98\% of the variance in current enrollment. This high persistence means that simple forecasting models based on the assumption that next year's enrollment will equal this year's enrollment perform as well as or better than more complex statistical and machine learning models. Specifically, the naïve persistence baseline achieved an average forecast error of 39.43 students across four out-of-sample test years, outperforming moving averages, Ridge regression, and Random Forest models. This result leads to the rejection of Hypothesis 1, which predicted that sophisticated models would substantially outperform simple baselines.

The driver analysis confirms that admissions funnel metrics (applications received, admissions granted) and pricing variables (net price) are statistically significant predictors of enrollment, providing partial support for Hypothesis 2. However, the practical importance of these drivers is limited by the overwhelming dominance of persistence. Institutional policies that affect admissions, pricing, or financial aid produce marginal enrollment changes rather than transformative shifts in the short run.

These findings have important implications for both theory and practice. Theoretically, they highlight enrollment persistence as a structural feature of higher education systems, driven by capacity constraints, demographic inertia, and institutional stability. They also underscore the limits of data-driven forecasting in contexts where simple patterns dominate complex relationships. Practically, they suggest that institutions should embrace simple forecasting models for operational planning, focus strategic interventions on long-term positioning rather than short-term optimization, and invest in data infrastructure that enables rapid response to emerging trends.

The study also acknowledges significant limitations, including its geographic scope, reliance on annual data, and inability to establish causal relationships. Future research should address these limitations through comparative international studies, higher-frequency data analysis, and causal inference methods. Additional directions for future work include incorporating external data sources, extending forecast horizons, and developing specialized models for subpopulations such as international students.

In sum, this dissertation contributes to the enrollment forecasting literature by demonstrating that simplicity often beats complexity in predictive accuracy, explaining why this is the case, and offering practical guidance for institutions navigating an uncertain enrollment landscape. The findings challenge the prevailing enthusiasm for machine learning and big data analytics in higher education, suggesting that the most valuable analytical investments may lie not in sophisticated models but in high-quality data systems, robust benchmarks, and strategic agility in response to change.

As the higher education sector faces mounting pressures from demographic shifts, technological disruption, and financial constraints, effective enrollment forecasting will remain a critical capability for institutional resilience and success. This study provides a foundation for that capability by identifying what works, explaining why it works, and charting a path forward for both researchers and practitioners.

