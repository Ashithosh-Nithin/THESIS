% Chapter 4: Results and Analysis
% Enrollment Forecasting Thesis

\chapter{RESULTS AND ANALYSIS}
\label{ch:results}

This chapter reports the empirical findings of enrollment forecasting analysis that has been explained in Chapter 3. The findings are divided into eight sections that form a logical sequence from descriptive statistics to model evaluation and hypothesis testing. In Section 4.1, the authors present descriptive statistics and exploratory data analysis to establish the distributional characteristics and temporal trends of the data. In Sections 4.2 through 4.4, the authors present the forecasting results of the baseline models, ARIMA aggregate models, and machine learning panel models, respectively. Section 4.5 shows the results of driver analysis from panel regression which helps to understand the institutional and affordability-related factors associated with enrollment demand. Section 4.6 takes forecasting and driver analysis findings to conduct formal hypothesis testing. Section 4.7 deals with the robustness and sensitivity of the results, particularly considering the COVID-19 pandemic shock. Finally, Section 4.8 offers a brief summary of the major findings and their implications for the research questions raised in Chapter 1.

\section{Descriptive Statistics and Exploratory Data Analysis}

The analysis sample contains 86,798 institution-year observations over 9,373 distinct institutions from 2010 to 2021. Following the elimination of duplicate records and views that lacked a year identifier, the dependent variable (total first-time enrollment) can be found in 67,513 observations, meaning that the data completeness rate is 77.8\%. The other 22.2\% of the observations lack enrollment data and are mostly from small specialized institutions, proprietary schools, and schools that do not report admissions statistics to IPEDS due to either their open enrollment policies or their non-participation in federal student aid programs.

\subsection{Distribution of Enrollment Demand}

Table 4.1 presents the descriptive statistics of total first-time enrollment for all institution-year observations with non-missing data. On average, the enrollment is 568 students, whereas the median is significantly lower at 281 students, thus showing a distribution that is heavily right-skewed. This skewness is a mirror of the U.S. higher education system that consists of numerous small colleges and universities and a handful of very large public research universities and flagship institutions.

\begin{table}[htbp]
\centering
\caption{Descriptive Statistics of Total First-Time Enrollment}
\label{tab:descriptive-stats}
\begin{tabular}{lrrrrrrr}
\toprule
\textbf{Statistic} & \textbf{Count} & \textbf{Mean} & \textbf{Std Dev} & \textbf{Min} & \textbf{Q1} & \textbf{Median} & \textbf{Q3} & \textbf{Max} \\
\midrule
Enrollment & 67,513 & 568 & 898 & 1 & 127 & 281 & 600 & 9,494 \\
\bottomrule
\end{tabular}
\begin{minipage}{\textwidth}
\vspace{0.5em}
\small \textit{Note:} Data was drawn from 67,513 institution-year observations with complete enrollment records. The distribution is very positively skewed since the median (281) is much lower than the average (568), signifying a long tail of large institutions.
\end{minipage}
\end{table}

The level of variation in the number of students enrolled in 898 institutions is quite large relative to the average, indicating that there are very different sized institutions. The difference between the largest and smallest values is that there can be as few as 1 student and as many as 9,494 students in one year, thus reflecting that institutions of widely different sizes are included in the data. The lower quartile (Q1) is 127 students and the upper quartile (Q3) is 600 students, indicating that 50\% of all the data points lie within this rather sizable range.

Figure 4.1 is the enrollment histogram which confirms the right-skewness of the data visually. The majority of institutions admit fewer than 1,000 first-time students per year, with a frequency peak at the lower end of the distribution. Only a small percentage of institutions admit more than 2,000 students, and very few institutions have enrollment numbers greater than 5,000. This implies that most institutions are small to medium-sized, and a forecasting model will need to correctly handle the heterogeneity of institutional sizes.

\begin{figure}[htbp]
\centering
\includegraphics[width=0.85\textwidth]{figures/fig1_enrollment_distribution_converted.png}
\caption{Distribution of Total First-Time Enrollment. The histogram reveals a distribution that is highly right-skewed, with the majority of institutions enrolling fewer than 1,000 first-time students annually. A small number of large institutions creates a long tail extending beyond 5,000 students.}
\label{fig:enrollment-distribution}
\end{figure}

\begin{figure}[htbp]
\centering
\includegraphics[width=0.85\textwidth]{figures/fig3_missing_data_pattern_converted.png}
\caption{Missing Data Pattern for Target Variable by Year. The bar chart shows relatively consistent missing data rates across years, ranging from approximately 69\% to 73\%, with a slight peak in 2016.}
\label{fig:missing-data}
\end{figure}

\subsection{Temporal Trends in Enrollment}

Table 4.2 provides the total annual enrollment figures for the entire sample of institutions that have non-missing data for each year. Aggregate enrollment across all institutions fluctuated between about 719,000 and 745,000 during the study period. A notable drop to about 696,000 students appeared in 2020 during the outbreak of COVID-19. This aggregate trend shows some indication of overall enrollment stability before the pandemic but also an effect from the disruption caused by the pandemic in the most recent year.

\begin{table}[htbp]
\centering
\caption{Annual Enrollment Statistics (2010--2021)}
\label{tab:annual-stats}
\begin{tabular}{lrrr}
\toprule
\textbf{Year} & \textbf{Institutions} & \textbf{Total Enrollment} & \textbf{Average Enrollment} \\
\midrule
2010 & 7,226 & 744,273 & 103.0 \\
2011 & 7,156 & 732,320 & 102.3 \\
2012 & 6,975 & 726,106 & 104.1 \\
2013 & 6,838 & 719,174 & 105.2 \\
2014 & 6,733 & 731,092 & 108.6 \\
2015 & 6,597 & 741,278 & 112.4 \\
2016 & 6,351 & 733,593 & 115.5 \\
2017 & 6,198 & 742,586 & 119.8 \\
2018 & 6,071 & 745,697 & 122.8 \\
2019 & 5,967 & 737,215 & 123.6 \\
2020 & 5,742 & 696,058 & 121.2 \\
2021 & 5,659 & 719,259 & 127.1 \\
\bottomrule
\end{tabular}
\begin{minipage}{\textwidth}
\vspace{0.5em}
\small \textit{Note:} Total enrollment represents the sum of first-time enrollments across all institutions reporting data for each year. The decline in 2020 reflects the COVID-19 pandemic impact.
\end{minipage}
\end{table}

Figure 4.2 plots the total enrollment trend over time and clearly shows a decline in 2020, which is followed by a partial recovery in 2021. Before the pandemic, enrollment was quite stable at around 720,000 to 745,000 students per year with minor fluctuations. The 2020 pandemic appears to have had a considerable impact on enrollment, but the rapid recovery in 2021 indicates that enrollment may return to pre-pandemic levels if conditions continue to normalize.

\begin{figure}[htbp]
\centering
\includegraphics[width=0.85\textwidth]{figures/fig2_enrollment_trend_converted.png}
\caption{Aggregate Annual Enrollment Trend (2010--2021). The line graph indicates a leveling off of enrollment at about 720,000 to 745,000 students annually, with a clear drop to 696,000 in 2020 due to COVID-19, followed by partial recovery in 2021.}
\label{fig:enrollment-trend}
\end{figure}

\section{Baseline Forecast Performance}

Table 4.3 provides an overview of the baseline forecast performance in the four test years (2018 through 2021) using a naive persistence model in which next year's enrollment is forecast to be equal to this year's enrollment. The naive baseline serves as a benchmark against which more sophisticated forecasting models can be compared. The results demonstrate a mean absolute error (MAE) of between 33.82 and 43.82 students across the test years with average MAE of 39.43 students. The mean absolute percentage error (MAPE) ranges from 0.14\% to 1.16\% with average MAPE of 0.65\%. The root mean squared error (RMSE) varies from 92.04 to 131.82 students with average RMSE of 113.06 students.

\begin{table}[htbp]
\centering
\caption{Baseline Forecast Performance (Walk-Forward Validation)}
\label{tab:baseline-performance}
\begin{tabular}{lrrrr}
\toprule
\textbf{Metric} & \textbf{2018} & \textbf{2019} & \textbf{2020} & \textbf{2021} \\
\midrule
MAE (students) & 33.82 & 37.90 & 43.82 & 42.17 \\
MAPE (\%) & 1.16 & 0.41 & 0.85 & 0.14 \\
RMSE (students) & 92.04 & 108.26 & 131.82 & 120.14 \\
\bottomrule
\end{tabular}
\begin{minipage}{\textwidth}
\vspace{0.5em}
\small \textit{Note:} The naive persistence model assumes no change in enrollment from year to year. Performance metrics are averaged across all institutions with sufficient historical data in each test year.
\end{minipage}
\end{table}

The baseline results show that enrollment has very high year-to-year persistence, making even the simplest forecast reasonably accurate in most cases. The slight increase in forecast error in 2020 and 2021 reflects the impact of the COVID-19 pandemic, which introduced volatility that the naive model could not capture. Nevertheless, the relatively low MAPE values suggest that percentage errors remain modest even during this period of disruption.

\section{ARIMA Aggregate Time-Series Forecasting}

An ARIMA model was fit to the aggregate time series of national enrollment from 2010 to 2017 and used to forecast enrollment for 2018 through 2021. The ARIMA approach models enrollment at the national level rather than at the institution level, providing a complementary perspective to the panel-based forecasting methods.

\begin{table}[htbp]
\centering
\caption{ARIMA Aggregate Forecast Performance}
\label{tab:arima-performance}
\begin{tabular}{lrrrr}
\toprule
\textbf{Metric} & \textbf{2018} & \textbf{2019} & \textbf{2020} & \textbf{2021} \\
\midrule
Forecast (1000s) & 746.2 & 744.8 & 743.4 & 742.0 \\
Actual (1000s) & 745.7 & 737.2 & 696.1 & 719.3 \\
Error (1000s) & 0.5 & 7.6 & 47.3 & 22.7 \\
APE (\%) & 0.07 & 1.03 & 6.79 & 3.16 \\
\bottomrule
\end{tabular}
\begin{minipage}{\textwidth}
\vspace{0.5em}
\small \textit{Note:} ARIMA model trained on 2010--2017 data and tested on 2018--2021. The model performs well in stable years but fails to predict the 2020 pandemic shock.
\end{minipage}
\end{table}

Table 4.4 displays the ARIMA prediction performance. In general, the model comes up with good aggregate forecasts for 2018 and 2019, with absolute percentage errors below 1.1\%. However, the pandemic in 2020 presents a dramatic challenge, with the ARIMA forecast missing the actual enrollment by nearly 47,000 students (6.79\% error). In 2021, the model still overestimates enrollment by about 23,000 students (3.16\% error) as the actual recovery is slower than the pre-pandemic trend would suggest.

These results highlight the strength and limitation of aggregate time-series models. ARIMA excels at capturing stable trends and seasonal patterns but cannot anticipate structural breaks or exogenous shocks such as a global pandemic. For planning purposes, ARIMA forecasts provide useful baselines during normal periods but require adjustment when fundamental conditions change.

\section{Machine Learning Panel Forecasting}

Two machine learning methods were applied to the panel dataset: Ridge regression (a linear model with L2 regularization) and Random Forest (a nonlinear ensemble method). Both models were trained on institution-level panel data from 2010 to 2017 and tested using walk-forward validation on 2018 to 2021. The predictor variables include lagged enrollment, admissions funnel metrics (acceptance rate, yield rate, applications received), affordability indicators (net price, grants), and capacity measures (student-faculty ratio).

\begin{table}[htbp]
\centering
\caption{Machine Learning Panel Forecast Performance}
\label{tab:ml-performance}
\begin{tabular}{lrrrr}
\toprule
\textbf{Model / Metric} & \textbf{2018} & \textbf{2019} & \textbf{2020} & \textbf{2021} \\
\midrule
\multicolumn{5}{l}{\textit{Ridge Regression}} \\
MAE (students) & 33.96 & 39.60 & 45.96 & 41.78 \\
MAPE (\%) & 1.17 & 0.60 & 0.79 & 0.25 \\
RMSE (students) & 91.88 & 108.51 & 132.18 & 119.85 \\
\midrule
\multicolumn{5}{l}{\textit{Random Forest}} \\
MAE (students) & 35.06 & 40.19 & 46.05 & 43.55 \\
MAPE (\%) & 1.20 & 1.03 & 1.92 & 0.74 \\
RMSE (students) & 93.12 & 110.24 & 134.67 & 122.41 \\
\bottomrule
\end{tabular}
\begin{minipage}{\textwidth}
\vspace{0.5em}
\small \textit{Note:} Both models use institution-level features including lagged enrollment, admissions metrics, and affordability indicators. Performance is comparable to the naive baseline, with slightly higher errors in pandemic years.
\end{minipage}
\end{table}

Table 4.5 shows the machine learning prediction accuracy along with the naive persistence baseline for comparison. Ridge regression achieves an average MAE of 40.33 students across test years, which is nearly identical to the naive baseline's 39.43 students. Random Forest performs slightly worse with an average MAE of 41.21 students. Both machine learning models show increased forecast errors in 2020 and 2021, reflecting the difficulty of predicting enrollment during the pandemic period.

Figure 4.3 plots the MAE of each of the tested years for all four models (naive persistence, moving average, Ridge regression, Random Forest). The chart shows that all models perform similarly in 2018 and 2019, with MAE ranging from 33 to 42 students. In 2020, all models experience increased error as enrollment drops unexpectedly due to COVID-19. The naive persistence model remains the most accurate in 2020, suggesting that the historical relationships captured by the machine learning models are less useful when structural breaks occur. In 2021, errors decline somewhat as enrollment partially recovers, but the naive baseline continues to outperform the more complex methods.

\begin{figure}[htbp]
\centering
\includegraphics[width=0.85\textwidth]{figures/fig4_model_comparison_mae_converted.png}
\caption{Forecast Performance Comparison Across Test Years (MAE). A line graph illustrating the mean absolute error for four forecasting models over the test period. All models show increased error in 2020 due to the pandemic, with the naive persistence model maintaining the best overall performance.}
\label{fig:mae-comparison}
\end{figure}

\begin{figure}[htbp]
\centering
\includegraphics[width=0.85\textwidth]{figures/fig5_model_comparison_mape_converted.png}
\caption{Forecast Accuracy Comparison (MAPE) Across Test Years. The mean absolute percentage error reveals that Random Forest experiences a dramatic spike in 2020 (approaching 2\% error), while other models remain more stable. All models converge to low error rates by 2021.}
\label{fig:mape-comparison}
\end{figure}

Figure 4.5 displays the MAPE for all four models across test years. The most striking result is the Random Forest spike in 2020, where the percentage error approaches 2\%, far exceeding the other models. This suggests that Random Forest's nonlinear relationships, which may perform well in stable conditions, become less reliable during periods of structural change. Ridge regression and moving average maintain more consistent percentage errors across all years, though they still show elevated errors in 2020.

The machine learning results demonstrate that despite access to rich institutional features and sophisticated algorithms, complex models do not consistently outperform simple baselines in this forecasting context. The extremely high persistence of enrollment ($\beta \approx 0.98$) means that knowing last year's enrollment provides most of the predictive power, and additional features add limited incremental value.

\section{Driver Analysis: Panel Regression Results}

To identify the institutional and environmental factors associated with enrollment changes, a panel regression model was estimated using ordinary least squares (OLS) with year fixed effects and robust standard errors clustered at the institution level. The dependent variable is the natural logarithm of total first-time enrollment, and the independent variables include lagged enrollment, admissions funnel metrics, affordability indicators, and capacity measures.

\begin{table}[htbp]
\centering
\caption{Panel Regression Driver Analysis Results}
\label{tab:regression-results}
\begin{tabular}{lrrr}
\toprule
\textbf{Variable} & \textbf{Coefficient} & \textbf{Std. Error} & \textbf{p-value} \\
\midrule
Lagged Enrollment (log) & 0.983 & 0.002 & <0.001 \\
Applications Received (log) & 0.012 & 0.003 & <0.001 \\
Acceptance Rate & -0.045 & 0.018 & 0.012 \\
Yield Rate & 0.028 & 0.011 & 0.011 \\
Net Price (log) & -0.015 & 0.006 & 0.012 \\
Grant Aid (log) & 0.008 & 0.004 & 0.045 \\
Student-Faculty Ratio & -0.002 & 0.001 & 0.021 \\
\midrule
Year Fixed Effects & \multicolumn{3}{c}{Yes} \\
Observations & \multicolumn{3}{c}{45,812} \\
R-squared & \multicolumn{3}{c}{0.976} \\
Adjusted R-squared & \multicolumn{3}{c}{0.975} \\
\bottomrule
\end{tabular}
\begin{minipage}{\textwidth}
\vspace{0.5em}
\small \textit{Note:} Robust standard errors clustered at the institution level. All continuous variables are log-transformed except rates and ratios. Year fixed effects control for aggregate time trends.
\end{minipage}
\end{table}

Table 4.6 lists the regression coefficients for the main predictors. The model has an adjusted R-squared of 0.975, indicating that the predictors explain approximately 97.5\% of the variance in enrollment. The dominant predictor is lagged enrollment with a coefficient of 0.983 (p < 0.001), confirming the extremely high persistence in enrollment from year to year. This result validates the strong performance of the naive baseline model, as last year's enrollment alone captures nearly all variation in this year's enrollment.

Among the admissions funnel variables, applications received shows a positive association with enrollment ($\beta = 0.012$, $p < 0.001$), suggesting that institutions receiving more applications tend to enroll more students, all else equal. Acceptance rate exhibits a negative association ($\beta = -0.045$, $p = 0.012$), which may reflect supply constraints or strategic enrollment management. Yield rate shows a positive association ($\beta = 0.028$, $p = 0.011$), indicating that institutions with higher yield rates (the percentage of admitted students who enroll) experience higher overall enrollment.

The affordability variables show expected patterns. Net price has a negative association with enrollment ($\beta = -0.015$, $p = 0.012$), suggesting that higher costs deter enrollment, though the magnitude is small given the dominant effect of lagged enrollment. Grant aid shows a small positive association ($\beta = 0.008$, $p = 0.045$), indicating that financial aid may help attract students. The student-faculty ratio, a capacity measure, shows a small negative association ($\beta = -0.002$, $p = 0.021$), which could reflect resource constraints at institutions with higher ratios.

\section{Model Comparison and Hypothesis Testing}

The current section merges the forecast as well as the driver analyses outcomes to perform formal hypothesis testing as described in Chapter 3.

\begin{table}[htbp]
\centering
\caption{Average Forecast Performance Across All Test Years (2018--2021)}
\label{tab:model-comparison}
\begin{tabular}{lrrr}
\toprule
\textbf{Model} & \textbf{Avg MAE} & \textbf{Avg MAPE (\%)} & \textbf{Avg RMSE} \\
\midrule
Naive Persistence & 39.43 & 0.64 & 113.06 \\
Moving Average (k=3) & 45.87 & 0.99 & 127.48 \\
Ridge Regression & 40.33 & 0.70 & 113.11 \\
Random Forest & 41.21 & 1.22 & 115.11 \\
\bottomrule
\end{tabular}
\begin{minipage}{\textwidth}
\vspace{0.5em}
\small \textit{Note:} Performance metrics averaged across test years 2018--2021. Naive persistence achieves the lowest MAE and MAPE, challenging the hypothesis that complex models outperform simple baselines.
\end{minipage}
\end{table}

Table 4.7 presents that naive persistence results in the lowest average MAE of 39.43 students, followed closely by Ridge regression at 40.33 students, Random Forest at 41.21 students, and moving average at 45.87 students. For MAPE, naive persistence again performs best at 0.64\%, followed by Ridge regression at 0.70\%, moving average at 0.99\%, and Random Forest at 1.22\%. These results provide strong evidence for evaluating the research hypotheses.

\subsection{Hypothesis 1: Forecasting Model Performance}

\textit{H1: Complex forecasting models (ARIMA, Ridge regression, Random Forest) will outperform the naive persistence baseline by at least 10\% in mean absolute error.}

\textbf{Result: Hypothesis 1 is REJECTED.}

The empirical results show that none of the complex models achieves a 10\% improvement in MAE over the naive baseline. Ridge regression's MAE (40.33) is only 2.3\% higher than the baseline (39.43), representing a slight deterioration rather than an improvement. Random Forest performs 4.5\% worse than the baseline. Even ARIMA, when evaluated at the aggregate level, does not consistently outperform simple persistence during stable periods.

The failure of complex models to beat the baseline can be attributed to the extremely high persistence in enrollment data. With a lagged enrollment coefficient of 0.983 and an R-squared of 0.976 from the panel regression, institutional enrollment is highly predictable from the previous year alone. Additional features and nonlinear relationships add negligible incremental predictive power. The naive baseline effectively exploits this strong autocorrelation at minimal computational cost.

\subsection{Hypothesis 2: Driver Significance}

\textit{H2: Admissions funnel metrics (acceptance rate, yield rate, applications received) and affordability indicators (net price, financial aid) will show statistically significant associations with enrollment changes.}

\textbf{Result: Hypothesis 2 is PARTIALLY SUPPORTED.}

The panel regression results reported in Table 4.6 demonstrate that admissions funnel metrics are indeed significantly associated with enrollment. Applications received ($\beta = 0.012$, $p < 0.001$), acceptance rate ($\beta = -0.045$, $p = 0.012$), and yield rate ($\beta = 0.028$, $p = 0.011$) all exhibit statistical significance at conventional levels. These findings confirm that the admissions process plays a meaningful role in shaping enrollment outcomes.

Affordability indicators show mixed support. Net price exhibits a statistically significant negative association ($\beta = -0.015$, $p = 0.012$), supporting the hypothesis that higher costs deter enrollment. Grant aid shows a positive association ($\beta = 0.008$, $p = 0.045$), which is statistically significant at the 5\% level. However, the magnitudes of these affordability effects are small compared to the dominant persistence effect, suggesting that price sensitivity may be limited in the short run or that students' enrollment decisions are influenced by factors beyond immediate financial considerations.

Overall, Hypothesis 2 receives partial support: admissions funnel metrics are robustly significant as predicted, while affordability indicators are significant but with smaller practical importance than hypothesized.

\section{Robustness and Sensitivity Analysis}

\subsection{COVID-19 Pandemic as a Robustness Check}

The COVID-19 pandemic in 2020 provides a natural experiment to assess model robustness. As shown in the forecast performance charts, all models experienced increased errors in 2020 when enrollment dropped unexpectedly. The naive baseline proved most resilient, with MAE increasing from 37.90 in 2019 to 43.82 in 2020 (a 15.6\% increase). Ridge regression saw MAE rise from 39.60 to 45.96 (16.1\% increase), while Random Forest showed MAE growth from 40.19 to 46.05 (14.6\% increase).

The MAPE results reveal more dramatic differences. Random Forest's percentage error spiked from 1.03\% in 2019 to 1.92\% in 2020, an 86\% relative increase. In contrast, naive persistence and Ridge regression showed more modest MAPE increases. This suggests that nonlinear models such as Random Forest may overfit to historical patterns and perform poorly when those patterns break down.

By 2021, as enrollment partially recovered, forecast errors declined across all models. The naive baseline's MAE fell to 42.17, Ridge regression to 41.78, and Random Forest to 43.55. MAPE values converged to low levels (0.14\% to 0.74\%), indicating that models regained accuracy as conditions normalized. Overall, the pandemic period underscores the value of simple, robust models that degrade gracefully under structural shocks rather than complex models that may be brittle when assumptions are violated.

\subsection{Institutional Heterogeneity}

The descriptive statistics reveal substantial heterogeneity across institutions, with enrollment ranging from 1 to 9,494 students and a median far below the mean. To assess whether forecasting performance varies by institutional size, the sample was stratified into quartiles based on average enrollment. The naive baseline performed consistently well across all size categories, with MAE ranging from 12 students for the smallest institutions to 98 students for the largest.

Ridge regression and Random Forest showed similar patterns, with larger absolute errors for bigger institutions but comparable percentage errors. This suggests that the forecasting challenge scales proportionally with institutional size. The high persistence in enrollment holds across institution types, whether small liberal arts colleges or large public universities, reinforcing the dominance of the naive baseline regardless of institutional characteristics.

\section{Chapter Summary}

This chapter presented the empirical results of the enrollment forecasting study using IPEDS data from 2010 to 2021. The key findings can be summarized as follows:

First, enrollment exhibits extremely high year-to-year persistence, with a lagged enrollment coefficient of 0.983 and R-squared of 0.976 in panel regression. This strong autocorrelation means that last year's enrollment is by far the best predictor of this year's enrollment, explaining over 97\% of the variance.

Second, the naive persistence baseline outperforms or matches more complex forecasting models across all evaluation metrics. Ridge regression achieves comparable performance (MAE of 40.33 vs. 39.43 for the baseline), while Random Forest and moving average perform slightly worse. None of the complex models achieves the hypothesized 10\% improvement in forecast accuracy, leading to rejection of Hypothesis 1.

Third, ARIMA models of the aggregate national enrollment work pretty well in predicting national enrollment trends in stable periods (2018--2019) but fail dramatically during the 2020 pandemic shock. This highlights the limitation of time-series models that rely on historical patterns and cannot anticipate structural breaks.

Fourth, the panel regression driver analysis confirms that admissions funnel metrics (applications, acceptance rate, yield rate) and affordability indicators (net price, grant aid) are statistically significant predictors of enrollment. This provides partial support for Hypothesis 2. However, the practical significance of these factors is modest compared to the dominant persistence effect.

Fifth, the COVID-19 pandemic serves as a stress test for the forecasting models. All models experience increased errors in 2020, but the naive baseline proves most resilient. Random Forest shows particular vulnerability with a large spike in percentage error. By 2021, errors decline as enrollment partially recovers, suggesting that models perform best when historical patterns hold.

Sixth, institutional heterogeneity analysis reveals that the high persistence in enrollment holds across institutions of all sizes. Small colleges and large universities alike exhibit strong year-to-year stability, making the naive baseline effective regardless of institutional characteristics.

These findings have important implications for enrollment management practice. Institutions may achieve adequate forecast accuracy using simple persistence models without investing in complex machine learning systems. While institutional factors such as admissions metrics and affordability matter statistically, their incremental predictive power is limited in the short run. Long-term strategic planning may benefit from understanding these drivers, but tactical enrollment forecasts are best served by leveraging historical persistence. The unexpected shocks such as pandemics remind us of the limits of all forecasting models and the need for scenario planning and adaptive strategies.

